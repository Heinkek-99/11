{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Heinkek-99/11/blob/master/GEMA_DA_Maching_Learning_devoir.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9ceff93-6231-437d-8788-0177c97231c4",
      "metadata": {
        "id": "d9ceff93-6231-437d-8788-0177c97231c4"
      },
      "source": [
        "#### Simulation de données"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6032fb08-4a5d-46d3-89fd-b44a0bbf1ef0",
      "metadata": {
        "id": "6032fb08-4a5d-46d3-89fd-b44a0bbf1ef0"
      },
      "source": [
        "[texte du lien](https://)Simuler la génération de y."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ca9409d-f8f7-4e8d-9c57-d1c192ac8194",
      "metadata": {
        "id": "0ca9409d-f8f7-4e8d-9c57-d1c192ac8194"
      },
      "source": [
        "$$\n",
        "y = 5*sin(\\frac{3}{7}*x)+ \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, \\frac{1}{4})\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c8a3cdd-43cb-49d8-bb54-e78a812d0809",
      "metadata": {
        "id": "9c8a3cdd-43cb-49d8-bb54-e78a812d0809"
      },
      "source": [
        "#### Méthode de kfold"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4cc7e136-46e5-4846-8154-8fd36c27d475",
      "metadata": {
        "id": "4cc7e136-46e5-4846-8154-8fd36c27d475"
      },
      "source": [
        "En utilisation la méthode k-fold de validation croisée et une méthode de régression polynômiale, réalisez un modèle de regression."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f26d36b-a7b3-495c-860d-637ce2782f75",
      "metadata": {
        "id": "8f26d36b-a7b3-495c-860d-637ce2782f75"
      },
      "source": [
        "#### Arbre de décision : donnée Titanic"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83acfeb1-f948-427e-9952-88fb9e5d60a1",
      "metadata": {
        "id": "83acfeb1-f948-427e-9952-88fb9e5d60a1"
      },
      "source": [
        "Dans ce jeu de donnée, supprimer les données. L'objectif est de construire un modèle pour prédire ceux qui ont survécu au naufrage ou pas. La variable cible est survie."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0f41ab7-19f6-4f11-b875-25bba2d71e1c",
      "metadata": {
        "id": "a0f41ab7-19f6-4f11-b875-25bba2d71e1c"
      },
      "source": [
        "#### Résumé en un paragraphe votre compréhension du compromis biais-variance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67d6c08b-c187-4513-a097-64889fbd8505",
      "metadata": {
        "id": "67d6c08b-c187-4513-a097-64889fbd8505"
      },
      "outputs": [],
      "source": [
        "# %% [markdown]\n",
        "# # Projet de Modélisation - Compromis Biais-Variance\n",
        "\n",
        "# %% [markdown]\n",
        "# ## Partie 1 : Régression Polynomiale avec Validation Croisée\n",
        "\n",
        "# %% [markdown]\n",
        "# ### Équation à modéliser :\n",
        "# $$ y = 5\\sin\\left(\\frac{3}{7}x\\right) + \\epsilon,\\quad \\epsilon \\sim \\mathcal{N}(0, 0.25) $$\n",
        "\n",
        "# %%\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Génération des données\n",
        "np.random.seed(42)\n",
        "x = np.linspace(0, 10, 100)\n",
        "y_true = 5 * np.sin((3/7)*x)\n",
        "y = y_true + np.random.normal(0, 0.5, size=len(x))\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.scatter(x, y, s=10, label='Données avec bruit')\n",
        "plt.plot(x, y_true, c='red', label='Vraie relation')\n",
        "plt.legend()\n",
        "plt.title('Génération des données synthétiques')\n",
        "plt.show()\n",
        "\n",
        "# %%\n",
        "# Validation croisée k-fold\n",
        "k = 5\n",
        "degrees = range(1, 11)\n",
        "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
        "\n",
        "mse_train = []\n",
        "mse_val = []\n",
        "\n",
        "for degree in degrees:\n",
        "    mse_train_fold = []\n",
        "    mse_val_fold = []\n",
        "\n",
        "    poly = PolynomialFeatures(degree=degree)\n",
        "    X_poly = poly.fit_transform(x.reshape(-1, 1))\n",
        "\n",
        "    for train_idx, val_idx in kf.split(X_poly):\n",
        "        X_train, X_val = X_poly[train_idx], X_poly[val_idx]\n",
        "        y_train, y_val = y[train_idx], y[val_idx]\n",
        "\n",
        "        model = LinearRegression()\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        mse_train_fold.append(mean_squared_error(y_train, model.predict(X_train)))\n",
        "        mse_val_fold.append(mean_squared_error(y_val, model.predict(X_val)))\n",
        "\n",
        "    mse_train.append(np.mean(mse_train_fold))\n",
        "    mse_val.append(np.mean(mse_val_fold))\n",
        "\n",
        "# %%\n",
        "# Visualisation des résultats\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(degrees, mse_train, 'o-', label='Erreur entraînement')\n",
        "plt.plot(degrees, mse_val, 'o-', label='Erreur validation')\n",
        "plt.xlabel('Degré polynomial')\n",
        "plt.ylabel('MSE')\n",
        "plt.xticks(degrees)\n",
        "plt.legend()\n",
        "plt.title('Courbe de complexité modèle')\n",
        "plt.show()\n",
        "\n",
        "# %%\n",
        "# Meilleur modèle\n",
        "best_degree = degrees[np.argmin(mse_val)]\n",
        "print(f\"Degré optimal : {best_degree}\")\n",
        "\n",
        "poly = PolynomialFeatures(degree=best_degree)\n",
        "X_poly = poly.fit_transform(x.reshape(-1, 1))\n",
        "model = LinearRegression().fit(X_poly, y)\n",
        "\n",
        "x_plot = np.linspace(0, 10, 500)\n",
        "X_plot = poly.transform(x_plot.reshape(-1, 1))\n",
        "y_pred = model.predict(X_plot)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.scatter(x, y, s=10, label='Données')\n",
        "plt.plot(x_plot, y_pred, c='red', label='Modèle optimal')\n",
        "plt.plot(x, y_true, '--', c='green', label='Vraie relation')\n",
        "plt.legend()\n",
        "plt.title(f'Régression polynomiale degré {best_degree}')\n",
        "plt.show()\n",
        "\n",
        "# %% [markdown]\n",
        "# ## Partie 2 : Arbre de Décision sur les données du Titanic\n",
        "\n",
        "# %%\n",
        "import pandas as pd\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "# Chargement des données\n",
        "titanic = pd.read_csv('Titanic - Titanic.csv')\n",
        "\n",
        "# Prétraitement\n",
        "titanic_clean = titanic.drop(columns=['nom'])\n",
        "titanic_clean['sexe'] = titanic_clean['sexe'].map({1: 0, 2: 1})  # Conversion en binaire\n",
        "titanic_clean['age'].fillna(titanic_clean['age'].median(), inplace=True)\n",
        "\n",
        "# Séparation des données\n",
        "X = titanic_clean[['classe', 'sexe', 'age', 'tarif']]\n",
        "y = titanic_clean['survie']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# %%\n",
        "# Entraînement du modèle\n",
        "tree = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "tree.fit(X_train, y_train)\n",
        "\n",
        "# Évaluation\n",
        "y_pred = tree.predict(X_test)\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n",
        "\n",
        "# Matrice de confusion\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Prédit')\n",
        "plt.ylabel('Réel')\n",
        "plt.title('Matrice de confusion')\n",
        "plt.show()\n",
        "\n",
        "# %%\n",
        "# Importance des caractéristiques\n",
        "feature_importance = pd.Series(tree.feature_importances_, index=X.columns)\n",
        "feature_importance.sort_values().plot(kind='barh')\n",
        "plt.title('Importance des caractéristiques')\n",
        "plt.show()\n",
        "\n",
        "# %% [markdown]\n",
        "# ## Partie 3 : Compromis Biais-Variance\n",
        "\n",
        "# %% [markdown]\n",
        "# **Analyse** :\n",
        "# - **Biais** : Erreur systématique due à des hypothèses simplificatrices (ex: modèle linéaire pour des données non-linéaires).\n",
        "# - **Variance** : Sensibilité aux fluctuations des données d'entraînement (ex: modèle trop complexe mémorise le bruit).\n",
        "#\n",
        "# Dans la régression polynomiale :\n",
        "# - Degré 1 : Biais élevé (sous-ajustement)\n",
        "# - Degré 10 : Variance élevée (surajustement)\n",
        "#\n",
        "# Pour le Titanic :\n",
        "# - Arbre profond : Variance élevée (règles trop spécifiques)\n",
        "# - Arbre peu profond : Biais élevé (règles trop générales)\n",
        "#\n",
        "# L'objectif est de trouver le point optimal où la somme biais² + variance est minimale, ce que permettent les techniques de validation croisée et de régularisation."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}